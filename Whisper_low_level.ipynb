{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hvo8QWN-a9"
      },
      "source": [
        "# Installing Whisper\n",
        "\n",
        "The commands below will install the Python packages needed to use Whisper models and evaluate the transcription results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZsJUxc0aRsAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0408e7a3-9f49-4e1e-c76d-e565e989a03d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-rozgw_rf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-rozgw_rf\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802823 sha256=5bba5ad822454bb5c10812adac4fd8b7cafc6711dfcaa19062eb3fc7d143504a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i1fzqffn/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.4 rapidfuzz-3.9.3\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "\n",
        "# 라이브러리 경로 출력\n",
        "print(whisper.__file__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4gDL-iP_owg",
        "outputId": "5acbbd02-ad51-44bd-d9f0-3c12a6074486"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RipYia1mCLSK",
        "outputId": "e6c17c9b-5a19-479b-90ad-635e4ffbc583"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import torch\n",
        "import torchaudio\n",
        "import whisper\n",
        "importlib.reload(whisper)\n",
        "import whisper.decoding as decoding_module  # 전체 모듈을 임포트\n",
        "importlib.reload(decoding_module)\n",
        "import whisper.model as whisper_model\n",
        "importlib.reload(whisper_model)\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "# 기본 디바이스 설정\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 모델 로드\n",
        "model = whisper.load_model(\"base\").to(DEVICE)\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")\n",
        "\n",
        "# 오디오 파일 로드\n",
        "audio_file = 'drive/MyDrive/연구프로젝트/data/voice/일반남여_일반통합01_F_KKJ00_47_수도권_녹음실_00003.wav'\n",
        "waveform, sample_rate = torchaudio.load(audio_file)\n",
        "\n",
        "# 오디오 파일이 모노 채널로 있는지 확인 (모노 채널로 변환)\n",
        "if waveform.size(0) > 1:\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "# 샘플 레이트를 16000Hz로 변환\n",
        "waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "sample_rate = 16000\n",
        "\n",
        "# Whisper가 요구하는 형식으로 변환\n",
        "start = time.time()\n",
        "waveform = waveform.flatten()\n",
        "waveform = whisper.pad_or_trim(waveform).to(DEVICE)\n",
        "print(\"waveform\", waveform.shape)\n",
        "mel = whisper.log_mel_spectrogram(waveform)\n",
        "print(\"melshape\", mel.shape)\n",
        "print(\"log_mel_time\", time.time() - start)\n",
        "# 모델 인퍼런스\n",
        "options = whisper.DecodingOptions(language=\"ko\")  # 원하는 옵션 설정\n",
        "start = time.time()\n",
        "result = model.decode(mel, options)\n",
        "print(\"model_time\", time.time() - start)\n",
        "# 결과 출력\n",
        "print(f\"Transcription: {result.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KpWeIqEBcZ7",
        "outputId": "db8c4523-5e89-4824-a5c7-b3d3ae29d832"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:04<00:00, 31.0MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is multilingual and has 71,825,920 parameters.\n",
            "waveform torch.Size([480000])\n",
            "melshape torch.Size([80, 3000])\n",
            "log_mel_time 0.7345175743103027\n",
            "model_time 2.0259196758270264\n",
            "Transcription: 만원 지하체를 담아면 하루종일 피곤해서 일을 못하겠더라고요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IMEkgyagYto"
      },
      "source": [
        "# Loading the LibriSpeech dataset\n",
        "\n",
        "The following will load the test-clean split of the LibriSpeech corpus using torchaudio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CqtR2Fi5-vP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuCCB2KYOJCE"
      },
      "outputs": [],
      "source": [
        "class LibriSpeech(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n",
        "    It will drop the last few seconds of a very small portion of the utterances.\n",
        "    \"\"\"\n",
        "    def __init__(self, split=\"test-clean\", device=DEVICE):\n",
        "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
        "            root=os.path.expanduser(\"~/.cache\"),\n",
        "            url=split,\n",
        "            download=True,\n",
        "        )\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
        "        assert sample_rate == 16000\n",
        "        audio = whisper.pad_or_trim(audio.flatten()).to(self.device)\n",
        "        mel = whisper.log_mel_spectrogram(audio)\n",
        "\n",
        "        return (mel, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YcRU5jqNqo2"
      },
      "outputs": [],
      "source": [
        "dataset = LibriSpeech(\"test-clean\")\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ljocCNuUAde"
      },
      "source": [
        "# Running inference on the dataset using a base Whisper model\n",
        "\n",
        "The following will take a few minutes to transcribe all utterances in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PokfNJtOYNu",
        "outputId": "2c53ec44-bc93-4107-b4fa-214e3f71fe8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is English-only and has 71,825,408 parameters.\n"
          ]
        }
      ],
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rY_7KV3_Ki_"
      },
      "outputs": [],
      "source": [
        "# predict without timestamps for short-form transcription\n",
        "options = whisper.DecodingOptions(language=\"ko\", without_timestamps=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "09a29a91f58d4462942505a3cc415801",
            "83391f98a240490987c397048fc1a0d4",
            "06b9aa5f49fa44ba8c93b647dc7db224",
            "da9c231ee67047fb89073c95326b72a5",
            "48da931ebe7f4fd299f8c98c7d2460ff",
            "7a901f447c1d477bb49f954e0feacedd",
            "39f5a6ae8ba74c8598f9c6d5b8ad2d65",
            "a0d10a42c753453283e5219c22239337",
            "09f4cb79ff86465aaf48b0de24869af9",
            "1b9cecf5b3584fba8258a81d4279a25b",
            "039b53f2702c4179af7e0548018d0588",
            "9df048b46f764cf68cbe0045b8ff73a8"
          ]
        },
        "id": "7OWTn_KvNk59",
        "outputId": "a813a792-3c91-4144-f11f-054fd6778023"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9df048b46f764cf68cbe0045b8ff73a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/164 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "for mels, texts in tqdm(loader):\n",
        "    results = model.decode(mels, options)\n",
        "    hypotheses.extend([result.text for result in results])\n",
        "    references.extend(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4nTyynELQ42j",
        "outputId": "1c72d25a-3e87-4c60-a8d1-1da9d2f73bd7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>reference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>He hoped there would be stew for dinner, turni...</td>\n",
              "      <td>HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stuffered into you, his belly counseled him.</td>\n",
              "      <td>STUFF IT INTO YOU HIS BELLY COUNSELLED HIM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>After early nightfall the yellow lamps would l...</td>\n",
              "      <td>AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello Bertie, any good in your mind?</td>\n",
              "      <td>HELLO BERTIE ANY GOOD IN YOUR MIND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Number 10. Fresh Nelly is waiting on you. Good...</td>\n",
              "      <td>NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2615</th>\n",
              "      <td>Oh, to shoot my soul's full meaning into futur...</td>\n",
              "      <td>OH TO SHOOT MY SOUL'S FULL MEANING INTO FUTURE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2616</th>\n",
              "      <td>Then I, long tried by natural ills, received t...</td>\n",
              "      <td>THEN I LONG TRIED BY NATURAL ILLS RECEIVED THE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2617</th>\n",
              "      <td>I love thee freely as men strive for right. I ...</td>\n",
              "      <td>I LOVE THEE FREELY AS MEN STRIVE FOR RIGHT I L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2618</th>\n",
              "      <td>I love thee with the passion put to use, in my...</td>\n",
              "      <td>I LOVE THEE WITH THE PASSION PUT TO USE IN MY ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2619</th>\n",
              "      <td>I love thee with the love I seemed to lose wit...</td>\n",
              "      <td>I LOVE THEE WITH A LOVE I SEEMED TO LOSE WITH ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2620 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             hypothesis  \\\n",
              "0     He hoped there would be stew for dinner, turni...   \n",
              "1          Stuffered into you, his belly counseled him.   \n",
              "2     After early nightfall the yellow lamps would l...   \n",
              "3                  Hello Bertie, any good in your mind?   \n",
              "4     Number 10. Fresh Nelly is waiting on you. Good...   \n",
              "...                                                 ...   \n",
              "2615  Oh, to shoot my soul's full meaning into futur...   \n",
              "2616  Then I, long tried by natural ills, received t...   \n",
              "2617  I love thee freely as men strive for right. I ...   \n",
              "2618  I love thee with the passion put to use, in my...   \n",
              "2619  I love thee with the love I seemed to lose wit...   \n",
              "\n",
              "                                              reference  \n",
              "0     HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...  \n",
              "1            STUFF IT INTO YOU HIS BELLY COUNSELLED HIM  \n",
              "2     AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...  \n",
              "3                    HELLO BERTIE ANY GOOD IN YOUR MIND  \n",
              "4     NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...  \n",
              "...                                                 ...  \n",
              "2615  OH TO SHOOT MY SOUL'S FULL MEANING INTO FUTURE...  \n",
              "2616  THEN I LONG TRIED BY NATURAL ILLS RECEIVED THE...  \n",
              "2617  I LOVE THEE FREELY AS MEN STRIVE FOR RIGHT I L...  \n",
              "2618  I LOVE THEE WITH THE PASSION PUT TO USE IN MY ...  \n",
              "2619  I LOVE THEE WITH A LOVE I SEEMED TO LOSE WITH ...  \n",
              "\n",
              "[2620 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPppEJRXX4ox"
      },
      "source": [
        "# Calculating the word error rate\n",
        "\n",
        "Now, we use our English normalizer implementation to standardize the transcription and calculate the WER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl-KBDflMhrg"
      },
      "outputs": [],
      "source": [
        "import jiwer\n",
        "from whisper.normalizers import EnglishTextNormalizer\n",
        "\n",
        "normalizer = EnglishTextNormalizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "6-O048q4WI4o",
        "outputId": "f2089bc9-f535-441e-f192-26e52ae82b5e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>reference</th>\n",
              "      <th>hypothesis_clean</th>\n",
              "      <th>reference_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>He hoped there would be stew for dinner, turni...</td>\n",
              "      <td>HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...</td>\n",
              "      <td>he hoped there would be stew for dinner turnip...</td>\n",
              "      <td>he hoped there would be stew for dinner turnip...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stuffered into you, his belly counseled him.</td>\n",
              "      <td>STUFF IT INTO YOU HIS BELLY COUNSELLED HIM</td>\n",
              "      <td>stuffered into you his belly counseled him</td>\n",
              "      <td>stuff it into you his belly counseled him</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>After early nightfall the yellow lamps would l...</td>\n",
              "      <td>AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...</td>\n",
              "      <td>after early nightfall the yellow lamps would l...</td>\n",
              "      <td>after early nightfall the yellow lamps would l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello Bertie, any good in your mind?</td>\n",
              "      <td>HELLO BERTIE ANY GOOD IN YOUR MIND</td>\n",
              "      <td>hello bertie any good in your mind</td>\n",
              "      <td>hello bertie any good in your mind</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Number 10. Fresh Nelly is waiting on you. Good...</td>\n",
              "      <td>NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...</td>\n",
              "      <td>number 10 fresh nelly is waiting on you good n...</td>\n",
              "      <td>number 10 fresh nelly is waiting on you good n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2615</th>\n",
              "      <td>Oh, to shoot my soul's full meaning into futur...</td>\n",
              "      <td>OH TO SHOOT MY SOUL'S FULL MEANING INTO FUTURE...</td>\n",
              "      <td>0 to shoot my soul is full meaning into future...</td>\n",
              "      <td>0 to shoot my soul is full meaning into future...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2616</th>\n",
              "      <td>Then I, long tried by natural ills, received t...</td>\n",
              "      <td>THEN I LONG TRIED BY NATURAL ILLS RECEIVED THE...</td>\n",
              "      <td>then i long tried by natural ills received the...</td>\n",
              "      <td>then i long tried by natural ills received the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2617</th>\n",
              "      <td>I love thee freely as men strive for right. I ...</td>\n",
              "      <td>I LOVE THEE FREELY AS MEN STRIVE FOR RIGHT I L...</td>\n",
              "      <td>i love thee freely as men strive for right i l...</td>\n",
              "      <td>i love thee freely as men strive for right i l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2618</th>\n",
              "      <td>I love thee with the passion put to use, in my...</td>\n",
              "      <td>I LOVE THEE WITH THE PASSION PUT TO USE IN MY ...</td>\n",
              "      <td>i love thee with the passion put to use in my ...</td>\n",
              "      <td>i love thee with the passion put to use in my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2619</th>\n",
              "      <td>I love thee with the love I seemed to lose wit...</td>\n",
              "      <td>I LOVE THEE WITH A LOVE I SEEMED TO LOSE WITH ...</td>\n",
              "      <td>i love thee with the love i seemed to lose wit...</td>\n",
              "      <td>i love thee with a love i seemed to lose with ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2620 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             hypothesis  \\\n",
              "0     He hoped there would be stew for dinner, turni...   \n",
              "1          Stuffered into you, his belly counseled him.   \n",
              "2     After early nightfall the yellow lamps would l...   \n",
              "3                  Hello Bertie, any good in your mind?   \n",
              "4     Number 10. Fresh Nelly is waiting on you. Good...   \n",
              "...                                                 ...   \n",
              "2615  Oh, to shoot my soul's full meaning into futur...   \n",
              "2616  Then I, long tried by natural ills, received t...   \n",
              "2617  I love thee freely as men strive for right. I ...   \n",
              "2618  I love thee with the passion put to use, in my...   \n",
              "2619  I love thee with the love I seemed to lose wit...   \n",
              "\n",
              "                                              reference  \\\n",
              "0     HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...   \n",
              "1            STUFF IT INTO YOU HIS BELLY COUNSELLED HIM   \n",
              "2     AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...   \n",
              "3                    HELLO BERTIE ANY GOOD IN YOUR MIND   \n",
              "4     NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...   \n",
              "...                                                 ...   \n",
              "2615  OH TO SHOOT MY SOUL'S FULL MEANING INTO FUTURE...   \n",
              "2616  THEN I LONG TRIED BY NATURAL ILLS RECEIVED THE...   \n",
              "2617  I LOVE THEE FREELY AS MEN STRIVE FOR RIGHT I L...   \n",
              "2618  I LOVE THEE WITH THE PASSION PUT TO USE IN MY ...   \n",
              "2619  I LOVE THEE WITH A LOVE I SEEMED TO LOSE WITH ...   \n",
              "\n",
              "                                       hypothesis_clean  \\\n",
              "0     he hoped there would be stew for dinner turnip...   \n",
              "1            stuffered into you his belly counseled him   \n",
              "2     after early nightfall the yellow lamps would l...   \n",
              "3                    hello bertie any good in your mind   \n",
              "4     number 10 fresh nelly is waiting on you good n...   \n",
              "...                                                 ...   \n",
              "2615  0 to shoot my soul is full meaning into future...   \n",
              "2616  then i long tried by natural ills received the...   \n",
              "2617  i love thee freely as men strive for right i l...   \n",
              "2618  i love thee with the passion put to use in my ...   \n",
              "2619  i love thee with the love i seemed to lose wit...   \n",
              "\n",
              "                                        reference_clean  \n",
              "0     he hoped there would be stew for dinner turnip...  \n",
              "1             stuff it into you his belly counseled him  \n",
              "2     after early nightfall the yellow lamps would l...  \n",
              "3                    hello bertie any good in your mind  \n",
              "4     number 10 fresh nelly is waiting on you good n...  \n",
              "...                                                 ...  \n",
              "2615  0 to shoot my soul is full meaning into future...  \n",
              "2616  then i long tried by natural ills received the...  \n",
              "2617  i love thee freely as men strive for right i l...  \n",
              "2618  i love thee with the passion put to use in my ...  \n",
              "2619  i love thee with a love i seemed to lose with ...  \n",
              "\n",
              "[2620 rows x 4 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"hypothesis_clean\"] = [normalizer(text) for text in data[\"hypothesis\"]]\n",
        "data[\"reference_clean\"] = [normalizer(text) for text in data[\"reference\"]]\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBGSITeBYPTT",
        "outputId": "7b3dbe7c-a37e-4a07-a50a-b27d5f88b68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER: 4.26 %\n"
          ]
        }
      ],
      "source": [
        "wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
        "\n",
        "print(f\"WER: {wer * 100:.2f} %\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "039b53f2702c4179af7e0548018d0588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06b9aa5f49fa44ba8c93b647dc7db224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d10a42c753453283e5219c22239337",
            "max": 164,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09f4cb79ff86465aaf48b0de24869af9",
            "value": 164
          }
        },
        "09a29a91f58d4462942505a3cc415801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83391f98a240490987c397048fc1a0d4",
              "IPY_MODEL_06b9aa5f49fa44ba8c93b647dc7db224",
              "IPY_MODEL_da9c231ee67047fb89073c95326b72a5"
            ],
            "layout": "IPY_MODEL_48da931ebe7f4fd299f8c98c7d2460ff"
          }
        },
        "09f4cb79ff86465aaf48b0de24869af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b9cecf5b3584fba8258a81d4279a25b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f5a6ae8ba74c8598f9c6d5b8ad2d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48da931ebe7f4fd299f8c98c7d2460ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a901f447c1d477bb49f954e0feacedd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83391f98a240490987c397048fc1a0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a901f447c1d477bb49f954e0feacedd",
            "placeholder": "​",
            "style": "IPY_MODEL_39f5a6ae8ba74c8598f9c6d5b8ad2d65",
            "value": "100%"
          }
        },
        "a0d10a42c753453283e5219c22239337": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da9c231ee67047fb89073c95326b72a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b9cecf5b3584fba8258a81d4279a25b",
            "placeholder": "​",
            "style": "IPY_MODEL_039b53f2702c4179af7e0548018d0588",
            "value": " 164/164 [05:08&lt;00:00,  1.86s/it]"
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}